{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annabelle2915/Machine-Learning-Projects/blob/main/BoneMarrowTransplantSuccessRateClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y38L9GUeQSu"
      },
      "source": [
        "# **Group 5: ITS61504 Assessment 3 August 2022**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Mining Coursework: This project aims to predict the success rate of\n",
        "bone marrow transplant towards patients\n"
      ],
      "metadata": {
        "id": "byzVi6dSaJ0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QGZfMPkLq8up"
      },
      "outputs": [],
      "source": [
        "# Line Wrapping in Collaboratory Google results\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "1NtwbJ-mxTFm",
        "outputId": "7a304bf4-3404-4fa8-f6a9-0b52cae2e4b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Mount Googlde Drive\n",
        "import io\n",
        "import requests\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvXZzPnafsEz"
      },
      "source": [
        "### **Q1 Data Preprocessing**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08lNGRjHicc_"
      },
      "source": [
        "**Data Understanding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11c9bdb8"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data understanding\n",
        "import pandas as pd # pandas libraries to manipulate or analyse data\n",
        "import numpy as np # numpy libraries for mathematical arrays functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsQXRgbsyc9p"
      },
      "outputs": [],
      "source": [
        "# Import bone_marrow.csv dataset to path variable from google drive\n",
        "path = \"/content/gdrive/MyDrive/Colab Notebooks/bone_marrow.csv\"\n",
        "\n",
        "# Assign dataset to dataframe named ori_df from variable path\n",
        "ori_df = pd.read_csv (path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh_oE0pjoxed"
      },
      "outputs": [],
      "source": [
        "#show basic information about the dataset such as entries, data types, etc.\n",
        "ori_df.info ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFquL6CoovdU"
      },
      "outputs": [],
      "source": [
        "# Show the top 30 rows of records from dataset corresonding its columns\n",
        "ori_df.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWT6OOP5u8fs"
      },
      "outputs": [],
      "source": [
        "# Show the statistical information of numerical variables in the dataset\n",
        "ori_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmLlRd9vICy"
      },
      "outputs": [],
      "source": [
        "# Show the statistical information of categorical variables in the dataset\n",
        "ori_df.describe(include='object')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF9srNVNlyXR"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "**Data Cleaning (Missing Value Identification)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_BXh0lntOeQ"
      },
      "outputs": [],
      "source": [
        "# Show if there's any duplicated records from the dataframe\n",
        "ori_df.duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYdTZFx7s0mF"
      },
      "outputs": [],
      "source": [
        "# Detect standard blank missing values in dataframe using pandas function\n",
        "ori_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNYrU5k62Vlu"
      },
      "outputs": [],
      "source": [
        "# Since there are no standard blank missing values for pandas function to detect,\n",
        "# but there's non standard missing records with '?' values seen such as at row 5 donor_CMV column,\n",
        "# we will need to identify the number of non standard blank missing values in this dataframe\n",
        "\n",
        "# Identify the non standard missing value of '?' in each column by descending order\n",
        "print(\"Number of non standard '?' missing value in the dataset: \")\n",
        "df_replaced = ori_df.replace(['?'], np.NaN)\n",
        "df_replaced.head(15)\n",
        "print(df_replaced.isna().sum().sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcWLFt8DxMkN"
      },
      "outputs": [],
      "source": [
        "# List the non standard missing values out in % for every records in the dataset\n",
        "for column in df_replaced.columns:\n",
        "  if df_replaced [column].isna().sum() != 0:\n",
        "    missing = df_replaced[column].isna().sum()\n",
        "    portion =(missing/df_replaced.shape [0]) * 100\n",
        "    print(f\"'{column}': Number of missing values '{missing}' ==>'{portion:.3f}%'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1XX438r0WI7"
      },
      "outputs": [],
      "source": [
        "# Every non standard missing values of '?' will need to be replaced to 'NaN' value\n",
        "# Replace \"?\" input with NaN\n",
        "df_replaced = ori_df.replace(['?'], np.nan)\n",
        "\n",
        "# Show the top 30 rows of records after the missing values of ? has been replaced\n",
        "df_replaced.head(30)\n",
        "# As seen at row 5 donor_CMV column, the '?' record has been replaced to 'NaN' value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning (Data Imputation)**"
      ],
      "metadata": {
        "id": "X7DrtyzM4g07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gHUMZut3axq"
      },
      "outputs": [],
      "source": [
        "# Replacing missing values with means and most frequent\n",
        "# change noise data like '?' to nan and change columns types to float\n",
        "# Replacing missing numerical values with mean\n",
        "# Must be converted from string to float i norder to get the mean.\n",
        "df_replaced['CD3_x1e8_per_kg'] = df_replaced['CD3_x1e8_per_kg'].astype(float)\n",
        "df_replaced['CD3_x1e8_per_kg'] = df_replaced['CD3_x1e8_per_kg'].fillna((df_replaced['CD3_x1e8_per_kg'].mean()))\n",
        "df_replaced['recipient_body_mass'] = df_replaced['recipient_body_mass'].astype(float)\n",
        "df_replaced['recipient_body_mass'] = df_replaced['recipient_body_mass'].fillna((df_replaced['recipient_body_mass'].mean()))\n",
        "df_replaced['CMV_status'] = df_replaced['CMV_status'].astype(float)\n",
        "df_replaced['CMV_status'] = df_replaced['CMV_status'].fillna((df_replaced['CMV_status'].mean()))\n",
        "df_replaced['antigen'] = df_replaced['antigen'].astype(float)\n",
        "df_replaced['antigen'] = df_replaced['antigen'].fillna((df_replaced['antigen'].mean()))\n",
        "df_replaced['allel'] = df_replaced['allel'].astype(float)\n",
        "df_replaced['allel'] = df_replaced['allel'].fillna((df_replaced['allel'].mean()))\n",
        "df_replaced['CD3_to_CD34_ratio'] = df_replaced['CD3_to_CD34_ratio'].astype(float)\n",
        "df_replaced['CD3_to_CD34_ratio'] = df_replaced['CD3_to_CD34_ratio'].fillna((df_replaced['CD3_to_CD34_ratio'].mean()))\n",
        "\n",
        "\n",
        "# Replacing missing cateogrical value with most frequent.\n",
        "df_replaced['donor_CMV'] = df_replaced['donor_CMV'].fillna(df_replaced['donor_CMV'].mode()[0])\n",
        "df_replaced['recipient_ABO'] = df_replaced['recipient_ABO'].fillna(df_replaced['recipient_ABO'].mode()[0])\n",
        "df_replaced['recipient_rh'] = df_replaced['recipient_rh'].fillna(df_replaced['recipient_rh'].mode()[0])\n",
        "df_replaced['recipient_CMV'] = df_replaced['recipient_CMV'].fillna(df_replaced['recipient_CMV'].mode()[0])\n",
        "df_replaced['ABO_match'] = df_replaced['ABO_match'].fillna(df_replaced['ABO_match'].mode()[0])\n",
        "df_replaced['extensive_chronic_GvHD'] = df_replaced['extensive_chronic_GvHD'].fillna(df_replaced['extensive_chronic_GvHD'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_replaced.head(10)"
      ],
      "metadata": {
        "id": "cFPiTNtg7S62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGIb6dbDc4h1"
      },
      "outputs": [],
      "source": [
        "df_replaced[50:80]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yeq0mhLfAuo"
      },
      "outputs": [],
      "source": [
        "# Verify that there are no non standard and standard missing data left after replacement\n",
        "print(\"Number of missing values by column after cleaning missing values:\\n\")\n",
        "print (df_replaced.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfL5L7eihrzg"
      },
      "outputs": [],
      "source": [
        "df_replaced.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0s5wORYtoYe"
      },
      "outputs": [],
      "source": [
        "import seaborn as sbn\n",
        "# Removing outliers for recipient_body_mass column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(data = df_replaced['recipient_body_mass'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"recipient_body_mass\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"recipient_body_mass\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"recipient_body_mass\"] < Lower_Fence) | (df_replaced[\"recipient_body_mass\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df1 = df_replaced[~((df_replaced[\"recipient_body_mass\"] < Lower_Fence) | (df_replaced[\"recipient_body_mass\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df1.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fmEwczxzcHn"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for allel column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['allel'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"allel\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"allel\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"allel\"] < Lower_Fence) | (df_replaced[\"allel\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df2 = df1[~((df1[\"allel\"] < Lower_Fence) | (df1[\"allel\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df2.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmJiUA551UZg"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for CD34_x1e6_per_kg column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['CD34_x1e6_per_kg'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"CD34_x1e6_per_kg\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"CD34_x1e6_per_kg\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"CD34_x1e6_per_kg\"] < Lower_Fence) | (df_replaced[\"CD34_x1e6_per_kg\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df3 = df2[~((df2[\"CD34_x1e6_per_kg\"] < Lower_Fence) | (df2[\"CD34_x1e6_per_kg\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df3.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YhUP6aI2lTE"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for CD3_x1e8_per_kg column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['CD3_x1e8_per_kg'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"CD3_x1e8_per_kg\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"CD3_x1e8_per_kg\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"CD3_x1e8_per_kg\"] < Lower_Fence) | (df_replaced[\"CD3_x1e8_per_kg\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df4 = df3[~((df3[\"CD3_x1e8_per_kg\"] < Lower_Fence) | (df3[\"CD3_x1e8_per_kg\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df4.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow2Wb0ZS3KEE"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for CD3_to_CD34_ratio column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['CD3_to_CD34_ratio'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"CD3_to_CD34_ratio\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"CD3_to_CD34_ratio\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"CD3_to_CD34_ratio\"] < Lower_Fence) | (df_replaced[\"CD3_to_CD34_ratio\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df5 = df4[~((df4[\"CD3_to_CD34_ratio\"] < Lower_Fence) | (df4[\"CD3_to_CD34_ratio\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df5.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3YiITd-3uZ8"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for ANC_recovery column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['ANC_recovery'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"ANC_recovery\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"ANC_recovery\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"ANC_recovery\"] < Lower_Fence) | (df_replaced[\"ANC_recovery\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df6 = df5[~((df5[\"ANC_recovery\"] < Lower_Fence) | (df5[\"ANC_recovery\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df6.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDghpf9v4XF6"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for PLT_recovery column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['PLT_recovery'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"PLT_recovery\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"PLT_recovery\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"PLT_recovery\"] < Lower_Fence) | (df_replaced[\"PLT_recovery\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df7 = df6[~((df6[\"PLT_recovery\"] < Lower_Fence) | (df6[\"PLT_recovery\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df7.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-NE9n8A5Kv2"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for time_to_acute_GvHD_III_IV column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['time_to_acute_GvHD_III_IV'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"time_to_acute_GvHD_III_IV\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"time_to_acute_GvHD_III_IV\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"time_to_acute_GvHD_III_IV\"] < Lower_Fence) | (df_replaced[\"time_to_acute_GvHD_III_IV\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df8 = df7[~((df7[\"time_to_acute_GvHD_III_IV\"] < Lower_Fence) | (df7[\"time_to_acute_GvHD_III_IV\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df8.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWoglPmW6WLY"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for time_to_acute_GvHD_III_IV column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['time_to_acute_GvHD_III_IV'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"time_to_acute_GvHD_III_IV\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"time_to_acute_GvHD_III_IV\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"time_to_acute_GvHD_III_IV\"] < Lower_Fence) | (df_replaced[\"time_to_acute_GvHD_III_IV\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df8 = df7[~((df7[\"time_to_acute_GvHD_III_IV\"] < Lower_Fence) | (df7[\"time_to_acute_GvHD_III_IV\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df8.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fUHnzn-6q5U"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for time_to_acute_GvHD_III_IV column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['time_to_acute_GvHD_III_IV'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"time_to_acute_GvHD_III_IV\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"time_to_acute_GvHD_III_IV\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"time_to_acute_GvHD_III_IV\"] < Lower_Fence) | (df_replaced[\"time_to_acute_GvHD_III_IV\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df8 = df7[~((df7[\"time_to_acute_GvHD_III_IV\"] < Lower_Fence) | (df7[\"time_to_acute_GvHD_III_IV\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df8.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h63AOPjx7HcJ"
      },
      "outputs": [],
      "source": [
        "# Removing outliers for survival_time column\n",
        "\n",
        "# Visualising data with a boxplot\n",
        "sbn.boxplot(df_replaced['survival_time'])\n",
        "\n",
        "# Compute the Interquartile Range (IQR)\n",
        "Q1 = df_replaced[\"survival_time\"].quantile(0.25)\n",
        "Q3 = df_replaced[\"survival_time\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print (\"IQR: %.2f\" %IQR)\n",
        "\n",
        "# Calculate the Lower and Upper Fence\n",
        "Lower_Fence = Q1 - (1.5 * IQR)\n",
        "print (\"Lower_Fence: %.2f\" %Lower_Fence)\n",
        "Upper_Fence = Q3 + (1.5 * IQR)\n",
        "print (\"Upper_Fence: %.2f\" %Upper_Fence)\n",
        "\n",
        "# Display Outliers\n",
        "print(\"Data Outliers: \\n\")\n",
        "print (df_replaced[((df_replaced[\"survival_time\"] < Lower_Fence) | (df_replaced[\"survival_time\"] > Upper_Fence))])\n",
        "\n",
        "# Removing Outliers and storing remaining data\n",
        "print(f\"ori_df shape: {df_replaced.shape}\\n\")\n",
        "df9 = df8[~((df8[\"survival_time\"] < Lower_Fence) | (df8[\"survival_time\"] > Upper_Fence))]\n",
        "print(f\"df_replaced shape after removed outliers : {df9.shape}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VoQCrvr8NiK"
      },
      "source": [
        "### **Q2 Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnXxndQ2lCWA"
      },
      "outputs": [],
      "source": [
        "# Import necesary plots libraries for visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# show box plot for numeric type columns\n",
        "for columns in df9.select_dtypes(exclude = 'object').drop(columns =['id','survival_status']).columns:\n",
        "    boxdata = df9 [[columns] + ['survival_status']]\n",
        "    sns.boxplot(data = boxdata,x='survival_status',y=columns)\n",
        "    plt.title(columns)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0OAMZAFEXen"
      },
      "outputs": [],
      "source": [
        "# show box plot for object type columns\n",
        "for columns in df9.select_dtypes(exclude = 'float').columns[1:-1]:\n",
        "    boxdf = df9[[columns] + ['survival_status']]\n",
        "    r =pd.crosstab(boxdf.survival_status,boxdf[columns]).unstack().reset_index()\n",
        "    sns.barplot(data = r,x=columns,y=0,hue='survival_status')\n",
        "    plt.title(columns)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "-lAUVESqtR90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which variables correlates with the target variable for feature selection of more than 0.5 r value\n",
        "import seaborn as sbn\n",
        "plt.figure(figsize=(12,10))\n",
        "cor=df9.corr()\n",
        "sbn.heatmap(cor, xticklabels=cor.columns, yticklabels=cor.columns, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eh2h0ZN_sNVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the variables that has pearson value of more than 0.3 for feature selection\n",
        "matrix = cor.corr()\n",
        "matrix = matrix.unstack()\n",
        "matrix = matrix[abs(matrix) >= 0.3]\n",
        "print(matrix. to_string())"
      ],
      "metadata": {
        "id": "-8cPDCDlIVEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YpXIdk1_Pzo"
      },
      "outputs": [],
      "source": [
        "# Dropping attributes not involved in the EDA and printing the attributes involved in the EDA\n",
        "plt.figure(figsize=(10,10))\n",
        "eda = df9.drop([\"id\", \"donor_age\", \"donor_age_below_35\", \"donor_ABO\", \"donor_CMV\", \"recipient_age_below_10\", \"recipient_age_int\", \"recipient_gender\", \"recipient_ABO\", \"recipient_rh\", \"recipient_CMV\",\n",
        "\"disease_group\", \"gender_match\", \"ABO_match\", \"CMV_status\", \"HLA_match\", \"HLA_mismatch\", \"HLA_group_1\", \"risk_group\",\n",
        "\"stem_cell_source\", \"tx_post_relapse\", \"acute_GvHD_II_III_IV\",\t\"acute_GvHD_III_IV\" , \"extensive_chronic_GvHD\", \"time_to_acute_GvHD_III_IV\"], axis=1)\n",
        "\n",
        "eda.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the r values and plotting a heatmap using pearson method\n",
        "plt.figure(figsize=(12,10))\n",
        "correlation = eda.corr(method='pearson')\n",
        "sbn.heatmap(correlation, xticklabels=correlation.columns, yticklabels=correlation.columns, annot=True)"
      ],
      "metadata": {
        "id": "SEWMvQ0b0IFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJ6QG7ezsNCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the variables that has pearson value of more than 0.7\n",
        "matrix = eda.corr()\n",
        "matrix = matrix.unstack()\n",
        "matrix = matrix[abs(matrix) >= 0.7]\n",
        "print(matrix)\n"
      ],
      "metadata": {
        "id": "7jqRHH85OWOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the pairplot of all 9 attributes\n",
        "import seaborn as sbn\n",
        "\n",
        "sbn.pairplot(eda)"
      ],
      "metadata": {
        "id": "x9oSS50sRQSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scatter plots for the  recipient age attribute and colouring them by their survival status due to achieving highest r value\n",
        "sbn.relplot(x=\"recipient_age\", y=\"recipient_body_mass\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"CD34_x1e6_per_kg\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"CD3_x1e8_per_kg\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"CD3_to_CD34_ratio\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"ANC_recovery\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"PLT_recovery\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"survival_time\", hue=\"survival_status\", data=eda)\n",
        "sbn.relplot(x=\"recipient_body_mass\", y=\"recipient_age\", hue=\"survival_status\", data=eda)\n"
      ],
      "metadata": {
        "id": "9TCWAf2pOMxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**According to the Scatterplot principle to determine whether if the variables has strong or negative relationship, below shows the criteria of it:**\n",
        "1) If the value of y increases with the value of x, then we can say that the variables have a positive correlation. (Ans: recipient_body_mass and recipient_age, ‘CD34_x1e6_per_kg’ and ‘CD3_x1e8_per_kg’, )\n",
        "\n",
        "2) If the value of y decreases with the value of x, then we can say that the variables have a negative correlation. (Ans: CD34_x1e6_per_kg )\n",
        "\n",
        "3) If the value of y changes randomly independent of x, then it is said to have a zero corelation. (Ans: ANC_recovery)"
      ],
      "metadata": {
        "id": "SfqnMR5UmfDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PLot histogram and dsitribution curve for the recipient age and recipient body mass\n",
        "sbn.distplot (eda [\"recipient_age\"])"
      ],
      "metadata": {
        "id": "Hf248jPZYM2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLot histogram and dsitribution curve for the recipient age and recipient body mass\n",
        "sbn.distplot (eda [\"recipient_body_mass\"])"
      ],
      "metadata": {
        "id": "KnAyc5tOY80z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkn16Euajxqt"
      },
      "source": [
        "### **Q3 Classification Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnpprKTCzjOn"
      },
      "source": [
        "**Ensure balance data on dependent variable (survival status)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ibydxpWI5fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BtdjX9jz2Ra"
      },
      "outputs": [],
      "source": [
        "pip install -U imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoEIAYbnz5n9"
      },
      "outputs": [],
      "source": [
        "# Check the imbalance value of the original form of dataset\n",
        "# assign all attributes excluding the survival status attribute to the variable x.\n",
        "x = ori_df.drop([\"survival_status\"], axis=1)\n",
        "\n",
        "# assign survival status attribute to the variable y.\n",
        "y = ori_df[\"survival_status\"]\n",
        "\n",
        "# print the values of the two classes under the survival status column\n",
        "print(y.value_counts())\n",
        "\n",
        "# plot a pie chart of for the survival status column\n",
        "y.value_counts().plot.pie(autopct=\"%.2f\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the preprocessed dataset of dependent variable to check for imbalance data\n",
        "\n",
        "# assign all attributes excluding the survival status attribute to the variable x.\n",
        "x = df9.drop([\"survival_status\"], axis=1)\n",
        "\n",
        "# assign survival status attribute to the variable y.\n",
        "y = df9[\"survival_status\"]\n",
        "\n",
        "# print the values of the two classes under the survival status column\n",
        "print(y.value_counts())\n",
        "\n",
        "# plot a pie chart for the survival status column\n",
        "y.value_counts().plot.pie(autopct=\"%.2f\")"
      ],
      "metadata": {
        "id": "qX8RxHWpRKn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "#create two different dataframe of majority and minority class\n",
        "df_majority = df9[(df9['survival_status']==0)]\n",
        "df_minority = df9[(df9['survival_status']==1)]\n",
        "# upsample minority class\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,    # sample with replacement\n",
        "                                 n_samples= 59, # to match majority class\n",
        "                                 random_state=42)  # reproducible results\n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_minority_upsampled, df_majority])"
      ],
      "metadata": {
        "id": "0Q6Jy9aPRdE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign all attributes excluding the survival status attribute to the variable x.\n",
        "x = df_upsampled.drop([\"survival_status\"], axis=1)\n",
        "\n",
        "# assign the survival status attribute to the variable y.\n",
        "y = df_upsampled[\"survival_status\"]\n",
        "\n",
        "# print the values of the two classes under the survival status column\n",
        "print(y.value_counts())\n",
        "\n",
        "# plot a pie chart for the survival status column\n",
        "y.value_counts().plot.pie(autopct=\"%.2f\")"
      ],
      "metadata": {
        "id": "4kKjGedUR4Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWRfxzYq1UzU"
      },
      "outputs": [],
      "source": [
        "# Read balanced dependent variable\n",
        "print(df_upsampled)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ID column will be removed for classification model\n",
        "print('[Data frame without id column:-]')\n",
        "no_id_bonemarrowDF = df_upsampled.drop('id', axis = 1)\n",
        "print (no_id_bonemarrowDF.head())"
      ],
      "metadata": {
        "id": "KIwie9_Q_ejx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMg55Xyw3fK-"
      },
      "source": [
        "**Data Transformation (Label Encoding)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9l8WUjF1fEY"
      },
      "outputs": [],
      "source": [
        "# import LabelEncoder from sklearn.preprocessing package\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Select Non-Numerical Columns\n",
        "categorical_col = no_id_bonemarrowDF.select_dtypes (exclude=[np.number]).columns\n",
        "print (categorical_col)\n",
        "print (no_id_bonemarrowDF[categorical_col].head())\n",
        "\n",
        "# Iterate through column of categorical data to convert to numeric data using LabelEncoder()\n",
        "label_encoder = LabelEncoder()\n",
        "for i in categorical_col:\n",
        "  no_id_bonemarrowDF[i] = label_encoder.fit_transform (no_id_bonemarrowDF[i])\n",
        "\n",
        "print(\"\\nLabel Encoder Data:\")\n",
        "print(no_id_bonemarrowDF.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Transformation (MinMax Scaler)**"
      ],
      "metadata": {
        "id": "A3M_vNHtIqOy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5skSNZa5Vog"
      },
      "outputs": [],
      "source": [
        "# Data Transformation with MinMax Scaler Method\n",
        "from sklearn import preprocessing\n",
        "\n",
        "minmax_scale = preprocessing.MinMaxScaler().fit_transform (no_id_bonemarrowDF)\n",
        "scaled_frame = pd.DataFrame (minmax_scale, columns = no_id_bonemarrowDF.columns)\n",
        "print (scaled_frame.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Shuffling**"
      ],
      "metadata": {
        "id": "fRDhCQk_JD3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebg7SfO94YeT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Checking the Number of Levels in dependent variable\n",
        "levels = len (pd.value_counts(scaled_frame['survival_status']))\n",
        "print ('There are {} levels in the survival status column'.format (levels))\n",
        "\n",
        "# Shuffle Rows Prior to Splitting Data into Features (X) and Outcome (Y) to avoid overfitting\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_shuffled = shuffle (scaled_frame, random_state=42)\n",
        "\n",
        "# Set the survival status attribute as the dependent variable to x\n",
        "# All other attributes are set as independent variables to y\n",
        "DV = 'survival_status'\n",
        "\n",
        "x = df_shuffled.drop (DV, axis=1)\n",
        "y = df_shuffled [DV]\n",
        "\n",
        "# Split data in 80:20 ratio. 80% for training, 20% for testing\n",
        "# random_state = 42 is used to select the 20% of the data set for testing randomly\n",
        "x_train, x_test, y_train, y_test = train_test_split (x, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print (y_train.head ())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_wQazu17980"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvFiCRFm5r9M"
      },
      "outputs": [],
      "source": [
        "# Import sklearn linear library to perform logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression model function will be assigned into LR model variable\n",
        "LRmodel=LogisticRegression()\n",
        "\n",
        "# Train the model using x_train and y_train of the dataset variables\n",
        "LRmodel.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiizMN165wY_"
      },
      "outputs": [],
      "source": [
        "# Predict and store survival status based on predictors (train and test dataset)\n",
        "lr_pred_train = LRmodel.predict(x_train)\n",
        "lr_pred_test = LRmodel.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw9NmYdu53hu"
      },
      "outputs": [],
      "source": [
        "# Print the predicted values of test dataset using logistic regression\n",
        "print(lr_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m48RMx5D5z12"
      },
      "outputs": [],
      "source": [
        "# Show the accuracy result of train and test dataset using logistic regression model\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy for train set: {0:0.4f}\".format(accuracy_score(y_train, lr_pred_train)))\n",
        "print(\"Accuracy for test set: {0:0.4f}\".format(accuracy_score(y_test, lr_pred_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score,  recall_score\n",
        "\n",
        "print('Accuracy: %0.4f' % accuracy_score(y_test, lr_pred_test))\n",
        "print('Precision: %0.4f' % precision_score(y_test, lr_pred_test))\n",
        "print('Recall: %0.4f' % recall_score(y_test, lr_pred_test))"
      ],
      "metadata": {
        "id": "tw6zNaJjeIlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5VbuXNk59Pl"
      },
      "outputs": [],
      "source": [
        "# Perfomance evaluation of the logistics regression model\n",
        "# via confusion matrix and classification report.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = pd.DataFrame (confusion_matrix(y_test, lr_pred_test))\n",
        "cm['Total'] = np.sum(cm,axis =1)\n",
        "cm = cm.append (np.sum(cm, axis=0),ignore_index=True)\n",
        "cm.columns = ['Predicted Survive', 'Predicted Did not survive', 'Total']\n",
        "cm = cm.set_index([['Actual Survive', 'Actual Did not survive', 'Total']])\n",
        "print(\"Confusion matrix:\")\n",
        "print (cm)\n",
        "\n",
        "# Print classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report (y_test, lr_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegression_cm = confusion_matrix(y_test, lr_pred_test)\n",
        "logisticRegression_cm_disp = ConfusionMatrixDisplay(confusion_matrix=logisticRegression_cm, display_labels=[\"Survive\", \"Did Not Survive\"])\n",
        "logisticRegression_cm_disp.plot()\n",
        "logisticRegression_cm_disp.ax_.set_title(\"Decision Tree Model\")"
      ],
      "metadata": {
        "id": "IJhwOt86WeIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import roc library function\n",
        "from sklearn.metrics import  roc_curve, auc\n",
        "lr_pred_train = LRmodel.predict(x_train)\n",
        "lr_pred_test = LRmodel.predict(x_test)\n",
        "\n",
        "# Get AUC and ROC\n",
        "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, lr_pred_train)\n",
        "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, lr_pred_test)\n",
        "\n",
        "train_roc_auc = auc(train_fpr, train_tpr)\n",
        "test_roc_auc = auc(test_fpr, test_tpr)\n",
        "\n",
        "print('AUC for train set: %0.4f' % train_roc_auc)\n",
        "print('AUC for test set: %0.4f' % test_roc_auc)"
      ],
      "metadata": {
        "id": "1oSaiDUnzZCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKf9k3gIBax3"
      },
      "outputs": [],
      "source": [
        "# Visualize ROC curve\n",
        "plt.figure(figsize=(7, 5), dpi=80)\n",
        "plt.plot(test_fpr,\n",
        "         test_tpr,\n",
        "         color='tomato',\n",
        "         label='ROC curve for test set (area = %0.4f)' % test_roc_auc)\n",
        "plt.plot(train_fpr,\n",
        "         train_tpr,\n",
        "         color='dodgerblue',\n",
        "         label='ROC curve for train set (area = %0.4f)' % train_roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve', fontsize=16)\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtSbVhtn8C_G"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWajQ4JO8NQs"
      },
      "outputs": [],
      "source": [
        "# Import decision tree model from sklearn library to perform decision tree classification\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1mrVBZu8Reu"
      },
      "outputs": [],
      "source": [
        "# Train the shuffled x_train and t_train for decision tree classification\n",
        "DecisionTree = DecisionTreeClassifier(random_state =42)\n",
        "DecisionTree.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEHJEvGl8rF6"
      },
      "outputs": [],
      "source": [
        "# Predict and store survival status based on predictors (train and test dataset)\n",
        "DecisionTree_pred_train = DecisionTree.predict(x_train)\n",
        "DecisionTree_pred_test = DecisionTree.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the predicted values of test dataset using decision tree\n",
        "print(DecisionTree_pred_test)"
      ],
      "metadata": {
        "id": "Mnxhqq_Kg1ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7c6-x8V9Jao"
      },
      "outputs": [],
      "source": [
        "# Show the accuracy result of train and test dataset using decision tree model\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy for train set: {0:0.4f}\".format(accuracy_score(y_train, DecisionTree_pred_train)))\n",
        "print(\"Accuracy for test set: {0:0.4f}\".format(accuracy_score(y_test, DecisionTree_pred_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14v6u1KO9qeN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score,  recall_score, roc_curve, auc\n",
        "\n",
        "print('Accuracy: %0.4f' % accuracy_score(y_test, DecisionTree_pred_test))\n",
        "print('Precision: %0.4f' % precision_score(y_test, DecisionTree_pred_test))\n",
        "print('Recall: %0.4f' % recall_score(y_test, DecisionTree_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFlgSROXXXQ5"
      },
      "outputs": [],
      "source": [
        "decisionTree_cm = confusion_matrix(y_test, DecisionTree_pred_test)\n",
        "decisionTree_cm_disp = ConfusionMatrixDisplay(confusion_matrix=decisionTree_cm, display_labels=[\"Survive\", \"Did Not Survive\"])\n",
        "decisionTree_cm_disp.plot()\n",
        "decisionTree_cm_disp.ax_.set_title(\"Decision Tree Model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsuH_L_a96Xa"
      },
      "outputs": [],
      "source": [
        "# Perfomance evaluation of the logistics regression model\n",
        "# via confusion matrix and classification report.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = pd.DataFrame (confusion_matrix(y_test, DecisionTree_pred_test))\n",
        "cm['Total'] = np.sum(cm,axis =1)\n",
        "cm = cm.append (np.sum(cm, axis=0),ignore_index=True)\n",
        "cm.columns = ['Predicted Survive', 'Predicted Did not survive', 'Total']\n",
        "cm = cm.set_index([['Actual Survive', 'Actual Did not survive', 'Total']])\n",
        "print(\"Confusion matrix:\")\n",
        "print (cm)\n",
        "\n",
        "# Print classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report (y_test, DecisionTree_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJUUYjp2-9nM"
      },
      "outputs": [],
      "source": [
        "DecisionTree_pred_train = DecisionTree.predict(x_train)\n",
        "DecisionTree_pred_test = DecisionTree.predict(x_test)\n",
        "\n",
        "# Get AUC and ROC\n",
        "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, DecisionTree_pred_train)\n",
        "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, DecisionTree_pred_test)\n",
        "\n",
        "train_roc_auc = auc(train_fpr, train_tpr)\n",
        "test_roc_auc = auc(test_fpr, test_tpr)\n",
        "\n",
        "print('AUC for train set: %0.4f' % train_roc_auc)\n",
        "print('AUC for test set: %0.4f' % test_roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcRudtC5_ZbL"
      },
      "outputs": [],
      "source": [
        "# Visualize ROC curve\n",
        "plt.figure(figsize=(7, 5), dpi=80)\n",
        "plt.plot(test_fpr,\n",
        "         test_tpr,\n",
        "         color='tomato',\n",
        "         label='ROC curve for test set (area = %0.4f)' % test_roc_auc)\n",
        "plt.plot(train_fpr,\n",
        "         train_tpr,\n",
        "         color='dodgerblue',\n",
        "         label='ROC curve for train set (area = %0.4f)' % train_roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve', fontsize=16)\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOFjC-w6_rLL"
      },
      "source": [
        "**Model Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE9O99jC_qOT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# Compute false positive rates, true positive rates and thresholds\n",
        "# based on the logistic regression, random forest and neural network\n",
        "# models' prediction.\n",
        "logm_test_fpr, logm_test_tpr, logm_test_thresholds = roc_curve(y_test, lr_pred_test)\n",
        "dt_test_fpr, dt_test_tpr, dt_test_thresholds = roc_curve(y_test, DecisionTree_pred_test)\n",
        "\n",
        "\n",
        "# Compute area under curve for all three prediction models.\n",
        "logm_auc = auc(logm_test_fpr, logm_test_tpr)\n",
        "dt_auc = auc(dt_test_fpr, dt_test_tpr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YauD0DCHCAxQ"
      },
      "outputs": [],
      "source": [
        "# Set figure size\n",
        "sbn.set(rc={\"figure.figsize\": (7, 6)})\n",
        "\n",
        "# Plot ROC curve for all three prediction models based on\n",
        "# respective false positive rate and true positive rate.\n",
        "plt.plot(logm_test_fpr, logm_test_tpr, label=\"Logistic regression model (AUC): {0:0.4f}\".format(logm_auc), marker=\".\")\n",
        "plt.plot(dt_test_fpr, dt_test_tpr, label=\"Decision Tree model (AUC): {0:0.4f}\".format(dt_auc), marker=\".\")\n",
        "\n",
        "\n",
        "# Plot line with 0.5 AUC (Random prediction)\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')\n",
        "\n",
        "# Labeling axis and graph title\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}